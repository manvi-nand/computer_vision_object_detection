{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMVOeM9y78pwZVN2IcfS2pD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"6d3a3cf3976b4132bd8d27b8e430a788":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_318a95a43581436580bfd598552ec34d","IPY_MODEL_c750710ec17040c6849cd7a5b3151e1e","IPY_MODEL_a81043ff875d486596c2ce4bf88bd6af"],"layout":"IPY_MODEL_64281c2522fa4f0c903c30207345466e"}},"318a95a43581436580bfd598552ec34d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2386f4b44a342288486d9d7d725ad6a","placeholder":"​","style":"IPY_MODEL_864d4bdacd954a5e9834d48473b6fa54","value":"100%"}},"c750710ec17040c6849cd7a5b3151e1e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_70f500a889c6475e89885566cdf4dec1","max":53,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b1850c79ffe1454587df08293ed83e2d","value":53}},"a81043ff875d486596c2ce4bf88bd6af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32e49c72699f452cb1da3e1689e3baac","placeholder":"​","style":"IPY_MODEL_edbb191d54ee4a9da0f3d88224af0005","value":" 53/53 [00:02&lt;00:00, 28.57it/s]"}},"64281c2522fa4f0c903c30207345466e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2386f4b44a342288486d9d7d725ad6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"864d4bdacd954a5e9834d48473b6fa54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"70f500a889c6475e89885566cdf4dec1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1850c79ffe1454587df08293ed83e2d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"32e49c72699f452cb1da3e1689e3baac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"edbb191d54ee4a9da0f3d88224af0005":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ee389f0012143159050475e9c70c811":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5c32b3d487b140088162931da9184cd8","IPY_MODEL_566bd61eec004d2d97d5f43403adf057","IPY_MODEL_f0801c71b2db4d72bf6d64a9fb0f53ff"],"layout":"IPY_MODEL_de9636e43bfa4f6aaa9afebc1b357bd8"}},"5c32b3d487b140088162931da9184cd8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7d10665a30f43a798c1cb4856c23859","placeholder":"​","style":"IPY_MODEL_249e470ff62f4c34aadba73f73e1b126","value":"100%"}},"566bd61eec004d2d97d5f43403adf057":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fa5db22d89944cdb11cebbbad61b894","max":13,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b5bf36af559245ad940e3ec315360cdd","value":13}},"f0801c71b2db4d72bf6d64a9fb0f53ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fe9672f0e6d4542886688a2e0c62977","placeholder":"​","style":"IPY_MODEL_2485aee219334dc6819b70cafe30cdc4","value":" 13/13 [00:03&lt;00:00,  6.39it/s]"}},"de9636e43bfa4f6aaa9afebc1b357bd8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7d10665a30f43a798c1cb4856c23859":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"249e470ff62f4c34aadba73f73e1b126":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4fa5db22d89944cdb11cebbbad61b894":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5bf36af559245ad940e3ec315360cdd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3fe9672f0e6d4542886688a2e0c62977":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2485aee219334dc6819b70cafe30cdc4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HU_xxPk0GqHg","executionInfo":{"status":"ok","timestamp":1706618936616,"user_tz":-330,"elapsed":4048,"user":{"displayName":"CS21B1027- Manvitha Nandyala","userId":"15476616746162460083"}},"outputId":"7847b088-8a10-462a-fef0-f891dafb4bb7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["!pip install ultralytics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rd7i2nshGyz1","executionInfo":{"status":"ok","timestamp":1706618942315,"user_tz":-330,"elapsed":5702,"user":{"displayName":"CS21B1027- Manvitha Nandyala","userId":"15476616746162460083"}},"outputId":"6e27278e-a320-4081-a613-45c52e0a780d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.1.8)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.0+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.16.0+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.1.1.post2209072238)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.47.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.11.17)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"]}]},{"cell_type":"code","source":["import os\n","import shutil\n","import random\n","\n","!pip install tqdm --upgrade\n","from tqdm.notebook import tqdm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"anfLygA9H9uQ","executionInfo":{"status":"ok","timestamp":1706618950378,"user_tz":-330,"elapsed":8072,"user":{"displayName":"CS21B1027- Manvitha Nandyala","userId":"15476616746162460083"}},"outputId":"67b450ec-7bcf-4ad7-c2a1-48e30292dc73"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"]}]},{"cell_type":"code","source":["train_path_img = \"/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/yolo_data/images/train/\"\n","train_path_label = \"/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/yolo_data/labels/train/\"\n","val_path_img = \"/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/yolo_data/images/val/\"\n","val_path_label = \"/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/yolo_data/labels/val/\"\n","test_path = \"/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/yolo_data/test\""],"metadata":{"id":"Mj4MTTOaI2Rj","executionInfo":{"status":"ok","timestamp":1706618950379,"user_tz":-330,"elapsed":23,"user":{"displayName":"CS21B1027- Manvitha Nandyala","userId":"15476616746162460083"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!ls '/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/data'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B2eisjFGJvdB","executionInfo":{"status":"ok","timestamp":1706618950379,"user_tz":-330,"elapsed":21,"user":{"displayName":"CS21B1027- Manvitha Nandyala","userId":"15476616746162460083"}},"outputId":"182ed059-7c4b-4be9-b604-094e1a734ac3"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["22d_f_120_6ty.jpg  22d_f_85_fr.txt    6ty_f_90_we.jpg\t fre_f_95_54r.txt   rt3_f_20_54r.jpg\n","22d_f_120_6ty.txt  22d_f_90.jpg       6ty_f_90_we.txt\t fr_f_100_we.jpg    rt3_f_20_54r.txt\n","22d_f_125_fre.jpg  22d_f_90.txt       fre_f_10_54r.jpg\t fr_f_100_we.txt    rt3_f_25_6ty.jpg\n","22d_f_125_fre.txt  54r_f_120.jpg      fre_f_10_54r.txt\t fr_f_105_6ty.jpg   rt3_f_25_6ty.txt\n","22d_f_125.jpg\t   54r_f_120.txt      fre_f_115.jpg\t fr_f_105_6ty.txt   rt3_f_35_rt3.jpg\n","22d_f_125.txt\t   54r_f_5_22d.jpg    fre_f_115.txt\t fr_f_10.jpg\t    rt3_f_35_rt3.txt\n","22d_f_135_we.jpg   54r_f_5_22d.txt    fre_f_145_6ty.jpg  fr_f_10.txt\t    rt3_f_95.jpg\n","22d_f_135_we.txt   54r_f_65.jpg       fre_f_145_6ty.txt  fr_f_130.jpg\t    rt3_f_95.txt\n","22d_f_15_rt3.jpg   54r_f_65.txt       fre_f_155_we.jpg\t fr_f_130.txt\t    we_f_100.jpg\n","22d_f_15_rt3.txt   54r_f_75.jpg       fre_f_155_we.txt\t fr_f_140_fre.jpg   we_f_100.txt\n","22d_f_165_fr.jpg   54r_f_75.txt       fre_f_190_22d.jpg  fr_f_140_fre.txt   we_f_105.jpg\n","22d_f_165_fr.txt   54r_f_75_we.jpg    fre_f_190_22d.txt  fr_f_15.jpg\t    we_f_105.txt\n","22d_f_180_6ty.jpg  54r_f_75_we.txt    fre_f_35.jpg\t fr_f_15.txt\t    we_f_115_fre.jpg\n","22d_f_180_6ty.txt  6ty_f_110_54r.jpg  fre_f_35.txt\t fr_f_160_fre.jpg   we_f_115_fre.txt\n","22d_f_185_6ty.jpg  6ty_f_110_54r.txt  fre_f_40_22d.jpg\t fr_f_160_fre.txt   we_f_150_we.jpg\n","22d_f_185_6ty.txt  6ty_f_130_22d.jpg  fre_f_40_22d.txt\t fr_f_40.jpg\t    we_f_150_we.txt\n","22d_f_200_rt3.jpg  6ty_f_130_22d.txt  fre_f_45_22d.jpg\t fr_f_40.txt\t    we_f_175_we.jpg\n","22d_f_200_rt3.txt  6ty_f_195_6ty.jpg  fre_f_45_22d.txt\t fr_f_45.jpg\t    we_f_175_we.txt\n","22d_f_25.jpg\t   6ty_f_195_6ty.txt  fre_f_5.jpg\t fr_f_45.txt\t    we_f_20.jpg\n","22d_f_25.txt\t   6ty_f_50_we.jpg    fre_f_5.txt\t fr_f_55.jpg\t    we_f_20.txt\n","22d_f_30_22d.jpg   6ty_f_50_we.txt    fre_f_60_fr.jpg\t fr_f_55.txt\t    we_f_30.jpg\n","22d_f_30_22d.txt   6ty_f_65_we.jpg    fre_f_60_fr.txt\t fr_f_70.jpg\t    we_f_30.txt\n","22d_f_55_22d.jpg   6ty_f_65_we.txt    fre_f_60.jpg\t fr_f_70.txt\t    we_f_50.jpg\n","22d_f_55_22d.txt   6ty_f_80_54r.jpg   fre_f_60.txt\t rt3_f_110.jpg\t    we_f_50.txt\n","22d_f_80.jpg\t   6ty_f_80_54r.txt   fre_f_70_rt3.jpg\t rt3_f_110.txt\n","22d_f_80.txt\t   6ty_f_85.jpg       fre_f_70_rt3.txt\t rt3_f_170_6ty.jpg\n","22d_f_85_fr.jpg    6ty_f_85.txt       fre_f_95_54r.jpg\t rt3_f_170_6ty.txt\n"]}]},{"cell_type":"code","source":["\n","'''\n","Split the dataset into train and test and creates the train.txt and test.tx with\n","the respective path of the images in each folder\n","'''\n","\n","def train_test_split(path,neg_path=None, split = 0.2):\n","    print(\"------ PROCESS STARTED -------\")\n","\n","\n","    files = list(set([name[:-4] for name in os.listdir(path)])) ## removing duplicate names i.e. counting only number of images\n","\n","\n","    print (f\"--- This folder has a total number of {len(files)} images---\")\n","    random.seed(42)\n","    random.shuffle(files)\n","\n","    test_size = int(len(files) * split)\n","    train_size = len(files) - test_size\n","\n","    ## creating required directories\n","\n","    os.makedirs(train_path_img, exist_ok = True)\n","    os.makedirs(train_path_label, exist_ok = True)\n","    os.makedirs(val_path_img, exist_ok = True)\n","    os.makedirs(val_path_label, exist_ok = True)\n","\n","\n","    ### ----------- copying images to train folder\n","    for filex in tqdm(files[:train_size]):\n","      if filex == 'classes':\n","          continue\n","      shutil.copy2(path + filex + '.jpg',f\"{train_path_img}/\" + filex + '.jpg' )\n","      shutil.copy2(path + filex + '.txt', f\"{train_path_label}/\" + filex + '.txt')\n","\n","\n","\n","    print(f\"------ Training data created with 80% split {len(files[:train_size])} images -------\")\n","\n","    if neg_path:\n","        neg_images = list(set([name[:-4] for name in os.listdir(neg_path)])) ## removing duplicate names i.e. counting only number of images\n","        for filex in tqdm(neg_images):\n","            shutil.copy2(neg_path+filex+ \".jpg\", f\"{train_path_img}/\" + filex + '.jpg')\n","\n","        print(f\"------ Total  {len(neg_images)} negative images added to the training data -------\")\n","\n","        print(f\"------ TOTAL Training data created with {len(files[:train_size]) + len(neg_images)} images -------\")\n","\n","\n","\n","    ### copytin images to validation folder\n","    for filex in tqdm(files[train_size:]):\n","      if filex == 'classes':\n","          continue\n","      # print(\"running\")\n","      shutil.copy2(path + filex + '.jpg', f\"{val_path_img}/\" + filex + '.jpg' )\n","      shutil.copy2(path + filex + '.txt', f\"{val_path_label}/\" + filex + '.txt')\n","\n","    print(f\"------ Testing data created with a total of {len(files[train_size:])} images ----------\")\n","\n","    print(\"------ TASK COMPLETED -------\")\n","\n","## spliting the data into train-test and creating train.txt and test.txt files\n","# train_test_split('/content/drive/MyDrive/custom_notebooks/yolo_data/')\n","\n","### for label_tag\n","#train_test_split('/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/data') ### without negative images\n","# train_test_split('./data/','./negative_images/') ### if you want to feed negative images\n","train_test_split('/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/data/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":170,"referenced_widgets":["6d3a3cf3976b4132bd8d27b8e430a788","318a95a43581436580bfd598552ec34d","c750710ec17040c6849cd7a5b3151e1e","a81043ff875d486596c2ce4bf88bd6af","64281c2522fa4f0c903c30207345466e","d2386f4b44a342288486d9d7d725ad6a","864d4bdacd954a5e9834d48473b6fa54","70f500a889c6475e89885566cdf4dec1","b1850c79ffe1454587df08293ed83e2d","32e49c72699f452cb1da3e1689e3baac","edbb191d54ee4a9da0f3d88224af0005","7ee389f0012143159050475e9c70c811","5c32b3d487b140088162931da9184cd8","566bd61eec004d2d97d5f43403adf057","f0801c71b2db4d72bf6d64a9fb0f53ff","de9636e43bfa4f6aaa9afebc1b357bd8","e7d10665a30f43a798c1cb4856c23859","249e470ff62f4c34aadba73f73e1b126","4fa5db22d89944cdb11cebbbad61b894","b5bf36af559245ad940e3ec315360cdd","3fe9672f0e6d4542886688a2e0c62977","2485aee219334dc6819b70cafe30cdc4"]},"id":"SlV7jmZyHG-3","executionInfo":{"status":"ok","timestamp":1706618955911,"user_tz":-330,"elapsed":5550,"user":{"displayName":"CS21B1027- Manvitha Nandyala","userId":"15476616746162460083"}},"outputId":"2b345cd8-4a77-4098-c8c2-25375abfd7c0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["------ PROCESS STARTED -------\n","--- This folder has a total number of 66 images---\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/53 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d3a3cf3976b4132bd8d27b8e430a788"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["------ Training data created with 80% split 53 images -------\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/13 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee389f0012143159050475e9c70c811"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["------ Testing data created with a total of 13 images ----------\n","------ TASK COMPLETED -------\n"]}]},{"cell_type":"code","source":["#creating the .yaml file for the paths\n","\n","!touch /content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/config.yaml"],"metadata":{"id":"0JXjns_ZJI_D","executionInfo":{"status":"ok","timestamp":1706618956560,"user_tz":-330,"elapsed":659,"user":{"displayName":"CS21B1027- Manvitha Nandyala","userId":"15476616746162460083"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from ultralytics import YOLO\n","import os\n","\n","model=YOLO('yolov8m.pt')\n","result=model.train(data='/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/config.yaml', epochs=5, imgsz=640)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZXkWRMiQITt","executionInfo":{"status":"ok","timestamp":1706619109559,"user_tz":-330,"elapsed":153001,"user":{"displayName":"CS21B1027- Manvitha Nandyala","userId":"15476616746162460083"}},"outputId":"2dd51d90-caf2-4733-b7da-72f84e214fe7"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics YOLOv8.1.8 🚀 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.pt, data=/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/config.yaml, epochs=5, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n","Overriding model.yaml nc=80 with nc=2\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n","  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n","  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n","  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n","  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n","  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n","  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n","  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n","  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n","  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n"," 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n"," 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n"," 22        [15, 18, 21]  1   3776854  ultralytics.nn.modules.head.Detect           [2, [192, 384, 576]]          \n","Model summary: 295 layers, 25857478 parameters, 25857462 gradients, 79.1 GFLOPs\n","\n","Transferred 469/475 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train2', view at http://localhost:6006/\n","Freezing layer 'model.22.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/yolo_data/labels/train.cache... 66 images, 0 backgrounds, 0 corrupt: 100%|██████████| 66/66 [00:00<?, ?it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/yolo_data/labels/train.cache... 66 images, 0 backgrounds, 0 corrupt: 100%|██████████| 66/66 [00:00<?, ?it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Plotting labels to runs/detect/train2/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mruns/detect/train2\u001b[0m\n","Starting training for 5 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["        1/5      6.94G       1.83      1.613      1.178         39        640: 100%|██████████| 5/5 [00:11<00:00,  2.28s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:05<00:00,  2.00s/it]"]},{"output_type":"stream","name":"stdout","text":["                   all         66        750      0.957      0.449      0.461      0.232\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["        2/5      7.11G      1.549      1.132      1.089         26        640: 100%|██████████| 5/5 [00:03<00:00,  1.33it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  1.66it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all         66        750      0.966      0.473      0.491      0.306\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["        3/5      7.11G       1.53     0.8429       1.04         23        640: 100%|██████████| 5/5 [00:04<00:00,  1.20it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:04<00:00,  1.46s/it]"]},{"output_type":"stream","name":"stdout","text":["                   all         66        750      0.886      0.673      0.703      0.347\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["        4/5      7.16G      1.418      0.875      1.004         24        640: 100%|██████████| 5/5 [00:03<00:00,  1.40it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:04<00:00,  1.53s/it]"]},{"output_type":"stream","name":"stdout","text":["                   all         66        750      0.844      0.685      0.725      0.386\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["        5/5      7.19G      1.477     0.7562     0.9849         52        640: 100%|██████████| 5/5 [00:03<00:00,  1.54it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:02<00:00,  1.00it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all         66        750      0.801      0.737      0.759      0.421\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","5 epochs completed in 0.024 hours.\n","Optimizer stripped from runs/detect/train2/weights/last.pt, 52.0MB\n","Optimizer stripped from runs/detect/train2/weights/best.pt, 52.0MB\n","\n","Validating runs/detect/train2/weights/best.pt...\n","Ultralytics YOLOv8.1.8 🚀 Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","Model summary (fused): 218 layers, 25840918 parameters, 0 gradients, 78.7 GFLOPs\n"]},{"output_type":"stream","name":"stderr","text":["                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:03<00:00,  1.28s/it]\n"]},{"output_type":"stream","name":"stdout","text":["                   all         66        750      0.803      0.737      0.759      0.421\n","                player         66        701      0.907      0.984      0.985      0.624\n","              football         66         49      0.699       0.49      0.533      0.217\n","Speed: 0.2ms preprocess, 10.7ms inference, 0.0ms loss, 7.2ms postprocess per image\n","Results saved to \u001b[1mruns/detect/train2\u001b[0m\n"]}]},{"cell_type":"code","source":["#testing the model with best.pt model and with a new image\n","\n","infer1=YOLO('/content/runs/detect/train/weights/best.pt')\n","\n","infer1.predict('/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/fr_f_130 copy.jpg', save=True ,save_txt=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bCpjbG17S2eG","executionInfo":{"status":"ok","timestamp":1706619111403,"user_tz":-330,"elapsed":1872,"user":{"displayName":"CS21B1027- Manvitha Nandyala","userId":"15476616746162460083"}},"outputId":"d8eae028-9c70-477d-adae-b5f2828f6ec0"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","image 1/1 /content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/fr_f_130 copy.jpg: 384x640 5 players, 117.4ms\n","Speed: 2.7ms preprocess, 117.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","Results saved to \u001b[1mruns/detect/predict3\u001b[0m\n","1 label saved to runs/detect/predict3/labels\n"]},{"output_type":"execute_result","data":{"text/plain":["[ultralytics.engine.results.Results object with attributes:\n"," \n"," boxes: ultralytics.engine.results.Boxes object\n"," keypoints: None\n"," masks: None\n"," names: {0: 'player', 1: 'football'}\n"," obb: None\n"," orig_img: array([[[110, 125, 121],\n","         [110, 125, 121],\n","         [110, 125, 121],\n","         ...,\n","         [155, 160, 158],\n","         [155, 160, 158],\n","         [155, 160, 158]],\n"," \n","        [[110, 125, 121],\n","         [110, 125, 121],\n","         [110, 125, 121],\n","         ...,\n","         [155, 160, 158],\n","         [155, 160, 158],\n","         [155, 160, 158]],\n"," \n","        [[110, 125, 121],\n","         [110, 125, 121],\n","         [110, 125, 121],\n","         ...,\n","         [155, 160, 158],\n","         [155, 160, 158],\n","         [155, 160, 158]],\n"," \n","        ...,\n"," \n","        [[ 26,  59,  44],\n","         [ 25,  58,  43],\n","         [ 25,  58,  43],\n","         ...,\n","         [ 22,  55,  41],\n","         [ 21,  54,  40],\n","         [ 20,  53,  39]],\n"," \n","        [[ 25,  58,  43],\n","         [ 25,  58,  43],\n","         [ 26,  60,  43],\n","         ...,\n","         [ 22,  55,  41],\n","         [ 21,  54,  40],\n","         [ 21,  54,  40]],\n"," \n","        [[ 25,  58,  43],\n","         [ 25,  58,  43],\n","         [ 26,  60,  43],\n","         ...,\n","         [ 22,  55,  41],\n","         [ 22,  55,  41],\n","         [ 21,  54,  40]]], dtype=uint8)\n"," orig_shape: (2160, 3840)\n"," path: '/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/fr_f_130 copy.jpg'\n"," probs: None\n"," save_dir: 'runs/detect/predict3'\n"," speed: {'preprocess': 2.6993751525878906, 'inference': 117.36512184143066, 'postprocess': 1.7633438110351562}]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["import locale\n","locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"kcX1YHLGYW2a","executionInfo":{"status":"ok","timestamp":1706619111403,"user_tz":-330,"elapsed":13,"user":{"displayName":"CS21B1027- Manvitha Nandyala","userId":"15476616746162460083"}},"outputId":"3b9ff689-1a07-44cd-cb73-f04a166ff11f"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'en_US.UTF-8'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["infer=YOLO('/content/runs/detect/train/weights/best.pt')\n","\n","infer.predict('/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/test', save=True, save_txt=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TrMwIHANVApI","executionInfo":{"status":"ok","timestamp":1706619114665,"user_tz":-330,"elapsed":3273,"user":{"displayName":"CS21B1027- Manvitha Nandyala","userId":"15476616746162460083"}},"outputId":"f4f0abee-f230-4237-a263-3c067a793971"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","image 1/4 /content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/test/Football_in_Bloomington,_Indiana,_1995.jpg: 448x640 7 players, 137.0ms\n","image 2/4 /content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/test/fr_f_130 copy.jpg: 384x640 5 players, 29.0ms\n","image 3/4 /content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/test/fr_f_140_fre copy.jpg: 384x640 13 players, 25.4ms\n","image 4/4 /content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/test/wynton.jpg: 448x640 10 players, 28.9ms\n","Speed: 3.8ms preprocess, 55.1ms inference, 2.3ms postprocess per image at shape (1, 3, 448, 640)\n","Results saved to \u001b[1mruns/detect/predict4\u001b[0m\n","4 labels saved to runs/detect/predict4/labels\n"]},{"output_type":"execute_result","data":{"text/plain":["[ultralytics.engine.results.Results object with attributes:\n"," \n"," boxes: ultralytics.engine.results.Boxes object\n"," keypoints: None\n"," masks: None\n"," names: {0: 'player', 1: 'football'}\n"," obb: None\n"," orig_img: array([[[ 24,  32,  45],\n","         [ 24,  27,  41],\n","         [ 25,  31,  44],\n","         ...,\n","         [ 73,  80,  89],\n","         [ 75,  85,  92],\n","         [ 63,  73,  80]],\n"," \n","        [[ 27,  33,  46],\n","         [ 29,  32,  46],\n","         [ 29,  32,  46],\n","         ...,\n","         [ 70,  79,  88],\n","         [ 69,  78,  87],\n","         [ 72,  81,  90]],\n"," \n","        [[ 32,  35,  49],\n","         [ 34,  37,  51],\n","         [ 33,  36,  50],\n","         ...,\n","         [ 69,  78,  87],\n","         [ 70,  81,  89],\n","         [ 69,  80,  88]],\n"," \n","        ...,\n"," \n","        [[ 64,  95,  94],\n","         [ 64,  95,  94],\n","         [ 64,  95,  94],\n","         ...,\n","         [ 72, 101,  98],\n","         [ 71, 100,  97],\n","         [ 70,  99,  96]],\n"," \n","        [[ 63,  94,  93],\n","         [ 63,  94,  93],\n","         [ 63,  94,  93],\n","         ...,\n","         [ 64,  95,  92],\n","         [ 65,  96,  93],\n","         [ 65,  96,  93]],\n"," \n","        [[ 62,  93,  92],\n","         [ 61,  92,  91],\n","         [ 61,  92,  91],\n","         ...,\n","         [ 65,  94,  91],\n","         [ 64,  93,  90],\n","         [ 60,  91,  88]]], dtype=uint8)\n"," orig_shape: (2328, 3420)\n"," path: '/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/test/Football_in_Bloomington,_Indiana,_1995.jpg'\n"," probs: None\n"," save_dir: 'runs/detect/predict4'\n"," speed: {'preprocess': 3.95965576171875, 'inference': 136.95359230041504, 'postprocess': 2.1512508392333984},\n"," ultralytics.engine.results.Results object with attributes:\n"," \n"," boxes: ultralytics.engine.results.Boxes object\n"," keypoints: None\n"," masks: None\n"," names: {0: 'player', 1: 'football'}\n"," obb: None\n"," orig_img: array([[[110, 125, 121],\n","         [110, 125, 121],\n","         [110, 125, 121],\n","         ...,\n","         [155, 160, 158],\n","         [155, 160, 158],\n","         [155, 160, 158]],\n"," \n","        [[110, 125, 121],\n","         [110, 125, 121],\n","         [110, 125, 121],\n","         ...,\n","         [155, 160, 158],\n","         [155, 160, 158],\n","         [155, 160, 158]],\n"," \n","        [[110, 125, 121],\n","         [110, 125, 121],\n","         [110, 125, 121],\n","         ...,\n","         [155, 160, 158],\n","         [155, 160, 158],\n","         [155, 160, 158]],\n"," \n","        ...,\n"," \n","        [[ 26,  59,  44],\n","         [ 25,  58,  43],\n","         [ 25,  58,  43],\n","         ...,\n","         [ 22,  55,  41],\n","         [ 21,  54,  40],\n","         [ 20,  53,  39]],\n"," \n","        [[ 25,  58,  43],\n","         [ 25,  58,  43],\n","         [ 26,  60,  43],\n","         ...,\n","         [ 22,  55,  41],\n","         [ 21,  54,  40],\n","         [ 21,  54,  40]],\n"," \n","        [[ 25,  58,  43],\n","         [ 25,  58,  43],\n","         [ 26,  60,  43],\n","         ...,\n","         [ 22,  55,  41],\n","         [ 22,  55,  41],\n","         [ 21,  54,  40]]], dtype=uint8)\n"," orig_shape: (2160, 3840)\n"," path: '/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/test/fr_f_130 copy.jpg'\n"," probs: None\n"," save_dir: 'runs/detect/predict4'\n"," speed: {'preprocess': 4.685878753662109, 'inference': 29.042482376098633, 'postprocess': 2.518177032470703},\n"," ultralytics.engine.results.Results object with attributes:\n"," \n"," boxes: ultralytics.engine.results.Boxes object\n"," keypoints: None\n"," masks: None\n"," names: {0: 'player', 1: 'football'}\n"," obb: None\n"," orig_img: array([[[178, 189, 187],\n","         [180, 191, 189],\n","         [179, 190, 188],\n","         ...,\n","         [ 95,  99,  94],\n","         [ 95,  99,  94],\n","         [ 95,  99,  94]],\n"," \n","        [[177, 188, 186],\n","         [180, 191, 189],\n","         [179, 190, 188],\n","         ...,\n","         [ 95,  99,  94],\n","         [ 95,  99,  94],\n","         [ 95,  99,  94]],\n"," \n","        [[178, 189, 187],\n","         [180, 191, 189],\n","         [179, 190, 188],\n","         ...,\n","         [ 95,  99,  94],\n","         [ 95,  99,  94],\n","         [ 95,  99,  94]],\n"," \n","        ...,\n"," \n","        [[  8,  39,  24],\n","         [  7,  38,  23],\n","         [  7,  38,  23],\n","         ...,\n","         [ 12,  38,  24],\n","         [ 12,  38,  24],\n","         [ 12,  38,  24]],\n"," \n","        [[  6,  37,  22],\n","         [  7,  38,  23],\n","         [  9,  40,  25],\n","         ...,\n","         [ 11,  37,  23],\n","         [ 11,  37,  23],\n","         [ 11,  37,  23]],\n"," \n","        [[  6,  37,  22],\n","         [  8,  39,  24],\n","         [ 11,  42,  27],\n","         ...,\n","         [ 11,  37,  23],\n","         [ 11,  37,  23],\n","         [ 11,  37,  23]]], dtype=uint8)\n"," orig_shape: (2160, 3840)\n"," path: '/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/test/fr_f_140_fre copy.jpg'\n"," probs: None\n"," save_dir: 'runs/detect/predict4'\n"," speed: {'preprocess': 3.5440921783447266, 'inference': 25.38752555847168, 'postprocess': 2.242565155029297},\n"," ultralytics.engine.results.Results object with attributes:\n"," \n"," boxes: ultralytics.engine.results.Boxes object\n"," keypoints: None\n"," masks: None\n"," names: {0: 'player', 1: 'football'}\n"," obb: None\n"," orig_img: array([[[34, 65, 66],\n","         [37, 68, 69],\n","         [38, 69, 70],\n","         ...,\n","         [17, 57, 56],\n","         [ 0, 32, 32],\n","         [ 0, 27, 25]],\n"," \n","        [[29, 63, 63],\n","         [31, 65, 65],\n","         [34, 68, 68],\n","         ...,\n","         [29, 68, 70],\n","         [13, 49, 49],\n","         [ 8, 45, 43]],\n"," \n","        [[29, 65, 65],\n","         [30, 66, 66],\n","         [31, 66, 69],\n","         ...,\n","         [23, 58, 61],\n","         [14, 48, 48],\n","         [10, 44, 44]],\n"," \n","        ...,\n"," \n","        [[59, 88, 65],\n","         [44, 73, 50],\n","         [42, 71, 48],\n","         ...,\n","         [30, 49, 28],\n","         [46, 65, 44],\n","         [36, 55, 34]],\n"," \n","        [[48, 77, 54],\n","         [47, 77, 52],\n","         [47, 76, 53],\n","         ...,\n","         [23, 42, 21],\n","         [38, 57, 36],\n","         [28, 47, 26]],\n"," \n","        [[24, 52, 32],\n","         [30, 59, 36],\n","         [37, 65, 45],\n","         ...,\n","         [24, 43, 22],\n","         [37, 57, 38],\n","         [25, 45, 26]]], dtype=uint8)\n"," orig_shape: (443, 660)\n"," path: '/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/test/wynton.jpg'\n"," probs: None\n"," save_dir: 'runs/detect/predict4'\n"," speed: {'preprocess': 2.89154052734375, 'inference': 28.856515884399414, 'postprocess': 2.1305084228515625}]"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["!git clone https://github.com/ultralytics/yolov5.git\n","%cd yolov5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wQ8jtw9wYViB","executionInfo":{"status":"ok","timestamp":1706619708396,"user_tz":-330,"elapsed":3270,"user":{"displayName":"CS21B1027- Manvitha Nandyala","userId":"15476616746162460083"}},"outputId":"969ec49e-a943-486b-fb55-d9b42b8edfe2"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yolov5'...\n","remote: Enumerating objects: 16403, done.\u001b[K\n","remote: Counting objects: 100% (297/297), done.\u001b[K\n","remote: Compressing objects: 100% (220/220), done.\u001b[K\n","remote: Total 16403 (delta 154), reused 161 (delta 77), pack-reused 16106\u001b[K\n","Receiving objects: 100% (16403/16403), 15.15 MiB | 15.68 MiB/s, done.\n","Resolving deltas: 100% (11187/11187), done.\n","/content/yolov5\n"]}]},{"cell_type":"code","source":["# Run YOLOv8 detection\n","!python /content/yolov5/detect.py --weights /content/runs/detect/train/weights/best.pt --img-size 640 --source \"/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/video (2160p).mp4\"\n","\n","# Compress the resulting video using ffmpeg\n","!ffmpeg -i \"/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/video (2160p).mp4\" -vcodec libx265 \"/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/football.mp4\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YJOJtiWIb_u5","executionInfo":{"status":"ok","timestamp":1706620511605,"user_tz":-330,"elapsed":798705,"user":{"displayName":"CS21B1027- Manvitha Nandyala","userId":"15476616746162460083"}},"outputId":"4398257d-3be3-4dcb-8624-954c612917b5"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/runs/detect/train/weights/best.pt'], source=/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/video (2160p).mp4, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n","\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['gitpython>=3.1.30', 'Pillow>=10.0.1'] not found, attempting AutoUpdate...\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.2.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mCollecting gitpython>=3.1.30\n","  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.4/196.4 kB 6.8 MB/s eta 0:00:00\n","Collecting Pillow>=10.0.1\n","  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.5/4.5 MB 23.8 MB/s eta 0:00:00\n","Collecting gitdb<5,>=4.0.1 (from gitpython>=3.1.30)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 192.4 MB/s eta 0:00:00\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython>=3.1.30)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Installing collected packages: smmap, Pillow, gitdb, gitpython\n","  Attempting uninstall: Pillow\n","    Found existing installation: Pillow 9.4.0\n","    Uninstalling Pillow-9.4.0:\n","      Successfully uninstalled Pillow-9.4.0\n","Successfully installed Pillow-10.2.0 gitdb-4.0.11 gitpython-3.1.41 smmap-5.0.1\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 8.2s, installed 2 packages: ['gitpython>=3.1.30', 'Pillow>=10.0.1']\n","\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","YOLOv5 🚀 v7.0-282-g9cdbd1de Python-3.10.12 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Model summary (fused): 218 layers, 25840918 parameters, 0 gradients, 78.7 GFLOPs\n","Traceback (most recent call last):\n","  File \"/content/yolov5/detect.py\", line 309, in <module>\n","    main(opt)\n","  File \"/content/yolov5/detect.py\", line 304, in main\n","    run(**vars(opt))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n","    return func(*args, **kwargs)\n","  File \"/content/yolov5/detect.py\", line 199, in run\n","    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n","KeyError: 1594\n","ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n","  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n","  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n","  libavutil      56. 70.100 / 56. 70.100\n","  libavcodec     58.134.100 / 58.134.100\n","  libavformat    58. 76.100 / 58. 76.100\n","  libavdevice    58. 13.100 / 58. 13.100\n","  libavfilter     7.110.100 /  7.110.100\n","  libswscale      5.  9.100 /  5.  9.100\n","  libswresample   3.  9.100 /  3.  9.100\n","  libpostproc    55.  9.100 / 55.  9.100\n","Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/video (2160p).mp4':\n","  Metadata:\n","    major_brand     : mp42\n","    minor_version   : 0\n","    compatible_brands: mp42mp41isomavc1\n","    creation_time   : 2019-09-12T21:18:37.000000Z\n","  Duration: 00:00:15.30, start: 0.000000, bitrate: 23843 kb/s\n","  Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709), 4096x2160, 23590 kb/s, 24 fps, 24 tbr, 24 tbn, 48 tbc (default)\n","    Metadata:\n","      creation_time   : 2019-09-12T21:18:37.000000Z\n","      handler_name    : L-SMASH Video Handler\n","      vendor_id       : [0][0][0][0]\n","      encoder         : AVC Coding\n","  Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 253 kb/s (default)\n","    Metadata:\n","      creation_time   : 2019-09-12T21:18:37.000000Z\n","      handler_name    : L-SMASH Audio Handler\n","      vendor_id       : [0][0][0][0]\n","File '/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/football.mp4' already exists. Overwrite? [y/N] ^C\n"]}]},{"cell_type":"code","source":["\"\"\"from ultralytics import YOLO\n","\n","# Load the trained YOLOv8 model\n","model = YOLO('/content/runs/detect/train/weights/best.pt')\n","\n","# Specify the path to the input video\n","video_path = '/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/video (2160p).mp4'\n","\n","# Make predictions on the video\n","results = model.predict(source=video_path)\n","\n","# Visualize the predictions\n","results.show()\n","\n","# Specify the path to the output video\n","#output_video_path = 'path_to_output_video.mp4'\n","\n","# Save the annotated video\n","#results.save(save_dir=output_video_path)\"\"\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":107},"id":"WwOQHxaRX5vH","executionInfo":{"status":"ok","timestamp":1706619218564,"user_tz":-330,"elapsed":9,"user":{"displayName":"CS21B1027- Manvitha Nandyala","userId":"15476616746162460083"}},"outputId":"1d13bf4d-463b-4a86-9c33-1ff1da38d025"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"from ultralytics import YOLO\\n\\n# Load the trained YOLOv8 model\\nmodel = YOLO('/content/runs/detect/train/weights/best.pt')\\n\\n# Specify the path to the input video\\nvideo_path = '/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/video (2160p).mp4'\\n\\n# Make predictions on the video\\nresults = model.predict(source=video_path)\\n\\n# Visualize the predictions\\nresults.show()\\n\\n# Specify the path to the output video\\n#output_video_path = 'path_to_output_video.mp4'\\n\\n# Save the annotated video\\n#results.save(save_dir=output_video_path)\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["import torch\n","from pathlib import Path\n","from IPython.display import display, HTML\n","\n","# Load YOLOv8 model from torch hub\n","model = torch.hub.load('ultralytics/yolov5:v5.0', 'yolov5s', pretrained=True)\n","\n","# Specify the path to the input video\n","video_path = '/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/video (2160p).mp4'\n","\n","# Perform inference on the video\n","results = model(video_path)\n","\n","# Save the annotated video in a compatible format\n","output_video_path = '/content/gdrive/MyDrive/computer_vision_eng/football_people_detection_YOLOV8/football.mov'\n","results.save(save_dir=output_video_path, save_all=True, codec='mp4v', fps=30)\n","\n","# Display the annotated video in the notebook\n","video_html = f\"\"\"\n","<video width=\"640\" height=\"360\" controls>\n","  <source src=\"{output_video_path}\" type=\"video/mp4\">\n","</video>\n","\"\"\"\n","display(HTML(video_html))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":633},"id":"pQKm9YNjZVJr","executionInfo":{"status":"error","timestamp":1706619491877,"user_tz":-330,"elapsed":3357,"user":{"displayName":"CS21B1027- Manvitha Nandyala","userId":"15476616746162460083"}},"outputId":"0bc42965-53ac-480e-d11f-7b6b7decf8fa"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/hub.py:294: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n","  warnings.warn(\n","Downloading: \"https://github.com/ultralytics/yolov5/zipball/v5.0\" to /root/.cache/torch/hub/v5.0.zip\n","/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]},{"output_type":"stream","name":"stdout","text":["Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 14.1M/14.1M [00:00<00:00, 192MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"error","ename":"Exception","evalue":"Cache maybe be out of date, try force_reload=True. See https://github.com/ultralytics/yolov5/issues/36 for help.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_v5.0/hubconf.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(name, pretrained, channels, classes, autoshape)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mattempt_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# download if not found locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mmsd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# model state_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1013\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m                 return _load(opened_zipfile,\n\u001b[0m\u001b[1;32m   1015\u001b[0m                              \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1421\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1422\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mmod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_module_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'SPPF' on <module 'models.common' from '/root/.cache/torch/hub/ultralytics_yolov5_v5.0/models/common.py'>","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-f83356445ba8>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load YOLOv8 model from torch hub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ultralytics/yolov5:v5.0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'yolov5s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Specify the path to the input video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m                                            verbose=verbose, skip_validation=skip_validation)\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_or_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36m_load_local\u001b[0;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_entry_from_hubconf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhub_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_v5.0/hubconf.py\u001b[0m in \u001b[0;36myolov5s\u001b[0;34m(pretrained, channels, classes, autoshape)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0myolov5s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# YOLOv5-small model https://github.com/ultralytics/yolov5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yolov5s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_v5.0/hubconf.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(name, pretrained, channels, classes, autoshape)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mhelp_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://github.com/ultralytics/yolov5/issues/36'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cache maybe be out of date, try force_reload=True. See %s for help.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mhelp_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: Cache maybe be out of date, try force_reload=True. See https://github.com/ultralytics/yolov5/issues/36 for help."]}]},{"cell_type":"code","source":[],"metadata":{"id":"X3NDh1QzcsU2"},"execution_count":null,"outputs":[]}]}